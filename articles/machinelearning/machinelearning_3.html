<h1>Réduction de dimension</h1>

<p><b>Objectifs</b>: décrire/visualiser des données, les décorréler, les débruiter, améliorer les performances d'un apprentissage</p>

<p><b>Méthodes</b>: feature  selection (garder les features qui sont le plus correlés à la target), feature  derivation (créer des nouvelles features), clustering (regrouper ensemble des données similaires)</p>

<p>Dans la suite, X est une matrice contenant l'ensemble des données dont chaque ligne correspond à une donnée.</p>

<br>

<h4>Linear Discriminant Analysis  LDA</h4>

<p><ul>
	<li>Sur notre base de données, calculer le vecteur moyen pour chacune des classes noté \(\mu_c\) et le vecteur moyen pour toutes les classes noté \(\mu\)</li>
	<li>Mesurer l'éparpillement (scatter) des données:</li>
	<ul>
		<li>scatter  matrix pour la classe \(i\): \(S_i = \sum_{x \in C_i} (x-\mu_i)(x-\mu_i)^T\)</li>
		<li>within-class  scatter: \(S_W = mean(S_i)\)</li>
		<li>between classes scatter: \(S_B = \sum_i (\mu_c - \mu) ( \mu_c - \mu)^T\)</li>
	</ul>
	<li>Objectif: trouver la projection \(W\) qui permet d'avoir le quotient \((W^T S_B W)/(W^T S_W W)\) aussi grand que possible</li>
	<li>Méthode (spectral decomposition):</li>
	<ul>
		<li>si \(w\) est un vecteur propre de \(S_W^{-1} S_B\), le quotient sera égal à sa valeur propre</li>
		<li>Calculer les vecteurs propres et valeurs propres de \(S_W^{-1} S_B\)</li>
		<li>Conserver uniquement les vecteurs propres des valeurs propres les plus grandes pour construire la matrice de projection \(W\) (chaque ligne est un vecteur propre)</li>
	</ul>
	<li>Appliquer la réduction à un vecteur \(x\): \(T = W x\)</li>
</ul></p>

<br>

<h4>Principal Component  Analysis  PCA</h4>

<p>Similaire à LDA mais pour des données non labellisées
<ul>
	<li>Initialisation: centrer et réduire les entrées (moyenne à 0 et variance à 1)</li>
	<li>Objectif: trouver la projection \(W\) qui permet de maximiser l'éparpillement des données: scatter  matrix  \(S = \sum_{x} W^T x^T x W\)</li>
	<li>Méthode:</li>
	<ul>
		<li>Calculer les vecteurs propres et valeurs propres de \(X^T X\)</li>
		<li>Conserver uniquement les vecteurs propres des valeurs propres les plus grandes pour construire la matrice de projection \(W\)</li>
	</ul>
	<li>Appliquer la réduction à un vecteur \(x\): \(T = W x\)</li>
</ul></p>

<p>Afin d'effectuer une transformation non linéaire lors de la réduction de dimension, un kernel peut être utilisé (similaire à l'utilisation de kernels pour les SVM).</p>

<br>

<h4>Independent  Component  Analysis  ICA</h4>

<p>Dans l'algorithme PCA, les composantes de la projection sont orthogonales \(b_i \cdot b_j = 0\) et non corrélées \(cov(b_i, b_j) = 0\). Dans ICA, il est de plus imposé que les composantes de la projection soient indépendantes entre elles: \(E[b_i, b_j] = E[b_i]E[b_j]\)</p>

