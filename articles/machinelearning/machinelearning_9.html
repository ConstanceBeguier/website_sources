<h1>La réduction de dimension</h1>

<p>La <b>réduction de dimension</b> prend en entrée des données dans un espace en grande dimension et les transforme en données dans un espace en plus petite dimension tout en gardant un maximum d'information sur les données initiales. Cela peut permettre de visualiser les données plus facilement et rendre ainsi leur interprétation plus facile. La réduction de dimension peut aussi être utilisée pour améliorer les performances des algorithmes d'apprentissage. Dans ce cas, la réduction de dimension est effectuée sur la base de données d'apprentissage ce qui permet de réduire le bruit sur ces données et l'apprentissage se fait ensuite sur les données réduites. Cela permet aussi d'accélérer l'apprentissage.</p>

<p>Il existe deux grandes approches pour faire de la réduction de dimension. La <b>sélection de caractéristiques</b> consiste à sélectionner certaines composantes. Supposons que nous avons un jeu de données contenant des caractéristiques de différents appartements (surface au sol, nombre de pièce, étages, volume, prix, ...). La sélection de caractéristiques nous fera surement retirer le volume car cette donnée est très corrélée à la surface au sol. Au contraire, <b>l'extraction de caractéristiques</b> consiste à créer de nouvelles variables plus pertinentes à partir des variables initiales. Suppossons que nos données sont des points dans un espace à deux dimensions, l'extraction de caractéristiques consiste à trouver une droite dans l'espace tel que si nous projetons nos données sur cet axe, la distance entre la donnée initiale et la donnée projetée est minimale. Pour obtenir, la donnée réduite, il suffit de définir un point origine sur cet axe et de retourner la coordonnée de la donnée projetée sur cet axe. Par exemple, si nous avons les notes en mathématiques et en français pour chaque étudiant d'une classe et que nous effectuons une réduction de dimension. Alors pour chaque étudiant nous allons obtenir une nouvelle note qui représentera les deux notes précédentes.</p>

<div class="text-center">
<img src="../../img/machinelearning/regression_lineaire.jpg" class="img-fluid" alt="regression_lineaire">
</div>

<br>

<h4>Analyse en Composantes Principales PCA</h4>

<p>L'<b>analyse en composantes principales</b> est l'algorithme de réduction de dimension le plus connu. Il est basé sur une extraction de caractéristiques.</p>

<p>Supposons que notre base de données contient K points dans un espace à N dimensions. Nous pouvons représenter notre base de données par une matrice M à K lignes et N colonnes dont chaque ligne représente un point de la base de données. La PCA s'effectue en trouvant un nouvel axe de projection à chaque étape. La première étape consiste à trouver un vecteur \(u_0\) tel que la projection des points sur ce vecteur ait une variance maximale. Puis le deuxième axe de projection est obtenue avec la même méthode en imposant que ce deuxième axe \(u_1\) soit orthogonal au premier axe \(u_0\). Si nous arrêtons ici, notre PCA nous permet de transformer les points de la base de données en point en dimension 2. La matrice de projection \(U\) est une matrice à 2 lignes et N colonnes dont la première ligne est égale à \(u_0\) et la deuxième ligne est égale à \(u_1\). Si nous voulons obtenir la réduction d'une donnée \(x\) représentée par un vecteur colonne à N dimensions, alors il suffit de calculer \(Ux\).</p>

<p>Pour obtenir les vecteurs de projection, il suffit de calculer les vecteurs propres et les valeurs propres de la matrice carrée symétrique réelle \(M^T M_). Les vecteurs de projections sont égaux aux vecteurs propres des valeurs propres les plus grandes.</p>

<p>Si nous voulons effectuer une transformation non linéaire lors de la réduction de dimension, un kernel peut être utilisé (similaire à l'utilisation de kernels pour les SVM).</p>

<p>Il est souvent nécessaire de faire un pré-traitement sur la base de données avant d'effectuer la réduction de dimension. Ce pré-traitement consiste à appliquer une fonction affine aux données afin que leur moyenne soit nulle (on dit qu'on centre les données) et que leur variance soit égal à 1 (on dit qu'on réduit les données). Cela permet d'éviter qu'une composante ayant une forte variance prenne trop de poids lors de la réduction de dimension.</p>

<br>

<h4>Analyse en Composantes Indépendantes ICA</h4>

<p>L'<b>analyse en composantes indépendantes</b> est similaire à l'analyse en composantes principales. Dans la PCA, les vecteurs de projections sont orthogonaux entre eux \(u_i \cdot u_j\) = 0. Dans l'ICA, il est de plus imposé que les vecteurs de projection soient indépendants entre eux \(E[u_i, u_j] = E[u_i] E[u_j]\).</p>