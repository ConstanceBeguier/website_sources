<h1>Apprentissage par renforcement</h1>

<p><ul>
	<li><b>Objectif</b>: maximiser la récompense totale des étapes successives</li>
	<li><b>Méthode</b></li>
	<ul>
		<li>A chaque étape, l'algorithme propose la prochaine action</li>
		<li>Il obtient une récompense pour cette action</li>
		<li>Il essaie d'en déduire qu'elles sont les bonnes et mauvaises actions à partir de chacun des états</li>
		<li>Afin de ne pas se retrouver dans un maximum local, l'algorithme renvoie de temps en temps une action aléatoire</li>
	</ul>
</ul></p>

<br>

<h4>Q-learning</h4>

<p>\(Q(s,a)\) est une estimation de la récompense lorsque nous appliquons l'action \(a\) à partir de l'état \(s\).
<ul>
	<li><b>Initialisation</b></li>
	<ul>
		<li>Initialiser \(Q(s,a)\) pour tout \(s\) et tout \(a\) par une petite valeur aléatoire</li>
		<li>Choisir aléatoirement un état initial \(s\)</li>
	</ul>
	<li><b>Algorithme:</b> Répéter les étapes suivantes jusqu'à convergence</li>
	<ul>
		<li>Sélectionner une action \(a\) à l'aide d'un soft-max sur \(Q(s,a)\) (le soft-max donne la probabilité de choisir chacune des actions \(a\) possible à partir de l'état \(s\))</li>
		<li>Effectuer l'action \(a\), obtenir la récompense \(r\) et atteindre le nouvel état \(s'\)</li>
		<li>Mise à jour de \(Q(s,a)\): \(Q(s,a) = (1-\mu) Q(s,a) + \mu (r + \gamma max_{a'} Q(s', a'))\) avec</li>
		<ul>
			<li>\(r\) la récompense reçue</li>
			<li>\(\mu\) la vitesse d'apprentissage comprise entre 0 et 1</li>
			<li>\(\gamma\) le facteur d'actualisation</li>
		</ul>
		<li>Nouvel état \(s = s'\)</li>
	</ul>
</ul></p>

<br>

<h4>Sarsa algorithme</h4>

<p>Identique au Q-learning sauf pour la mise à jour de \(Q(s,a)\): 
$$Q(s,a) = (1 - \mu) Q(s,a) + \mu (r + \gamma Q(s', a'))$$ 
où \(a'\) est l'action effectuée sur le prochain état en suivant la même méthode (soft-max)</p>
